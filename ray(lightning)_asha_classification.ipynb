{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using Ray package\n",
        "* Hyperpatameter tuning is done using Asynchronous Successive Halving Scheduler(ASHA)\n",
        "* **Model** : a classification model built with lightning\n",
        "* **Dataset used for training the model** : MNIST\n"
      ],
      "metadata": {
        "id": "qazEaMzmDHoZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOXUg6ZxCaeQ"
      },
      "outputs": [],
      "source": [
        "!pip install \"ray[tune]\" torch torchvision pytorch-lightning==1.9.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.setrecursionlimit(30000)"
      ],
      "metadata": {
        "id": "MuvEcYiTCpAs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from filelock import FileLock\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import os\n"
      ],
      "metadata": {
        "id": "F0Y3viftCqJo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from ray import train, tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.tune.integration.pytorch_lightning import (\n",
        "    TuneReportCallback,\n",
        "    TuneReportCheckpointCallback,\n",
        ")\n"
      ],
      "metadata": {
        "id": "QfRkH8M1Cqln"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Model With Lightning\n",
        "Built a Classification Model Class from LightningModule"
      ],
      "metadata": {
        "id": "dYuvc1VKCzsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "    def __init__(self, config, data_dir=None):\n",
        "        super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "        self.data_dir = data_dir or os.getcwd()\n",
        "\n",
        "        self.layer_1_size = config[\"layer_1_size\"]\n",
        "        self.layer_2_size = config[\"layer_2_size\"]\n",
        "        self.lr = config[\"lr\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "\n",
        "        # mnist images are (1, 28, 28) (channels, width, height)\n",
        "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
        "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
        "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, width, height = x.size()\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        x = self.layer_1(x)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        x = self.layer_2(x)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        x = self.layer_3(x)\n",
        "        x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def cross_entropy_loss(self, logits, labels):\n",
        "        return F.nll_loss(logits, labels)\n",
        "\n",
        "    def accuracy(self, logits, labels):\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        accuracy = correct / len(labels)\n",
        "        return torch.tensor(accuracy)\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x, y = train_batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "\n",
        "        self.log(\"ptl/train_loss\", loss)\n",
        "        self.log(\"ptl/train_accuracy\", accuracy)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y = val_batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
        "        self.log(\"ptl/val_loss\", avg_loss)\n",
        "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
        "\n",
        "    # def on_validation_epoch_end(self):\n",
        "    #     avg_loss = torch.stack([x[\"val_loss\"] for x in self.validation_step]).mean()\n",
        "    #     avg_acc = torch.stack([x[\"val_accuracy\"] for x in self.validation_step]).mean()\n",
        "    #     self.log(\"ptl/val_loss\", avg_loss)\n",
        "    #     self.log(\"ptl/val_accuracy\", avg_acc)\n",
        "\n",
        "    @staticmethod\n",
        "    def download_data(data_dir):\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "        )\n",
        "        with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
        "            return MNIST(data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        mnist_train = self.download_data(self.data_dir)\n",
        "\n",
        "        self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=int(self.batch_size))\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=int(self.batch_size))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "# def train_mnist(config):\n",
        "#     model = LightningMNISTClassifier(config)\n",
        "#     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=True)\n",
        "\n",
        "#     trainer.fit(model)\n"
      ],
      "metadata": {
        "id": "RhdzvQIRCxWp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning the model parameters"
      ],
      "metadata": {
        "id": "0lrIHRGLF03i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "def train_mnist_tune(config, num_epochs=10, num_gpus=0, data_dir=\"~/data\"):\n",
        "    data_dir = os.path.expanduser(data_dir)\n",
        "    model = LightningMNISTClassifier(config, data_dir)\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=num_epochs,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        gpus=math.ceil(num_gpus),\n",
        "        logger=TensorBoardLogger(save_dir=os.getcwd(), name=\"\", version=\".\"),\n",
        "        enable_progress_bar=True,\n",
        "        callbacks=[\n",
        "            TuneReportCallback(\n",
        "                {\"loss\": \"ptl/val_loss\", \"mean_accuracy\": \"ptl/val_accuracy\"},\n",
        "                on=\"validation_end\",\n",
        "            )\n",
        "        ],\n",
        "    )\n",
        "    trainer.fit(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "qniDxvlgGsXs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_mnist_asha(num_samples=5, num_epochs=3, gpus_per_trial=1, data_dir=\"~/data\"):\n",
        "    # Configuring the search space\n",
        "    config = {\n",
        "        \"layer_1_size\": tune.choice([32, 64, 128]),\n",
        "        \"layer_2_size\": tune.choice([64, 128, 256]),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([32, 64, 128]),\n",
        "    }\n",
        "    # Selecting a scheduler: ASHA\n",
        "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2, brackets=5)\n",
        "    # Report\n",
        "    reporter = CLIReporter(\n",
        "        parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"],\n",
        "    )\n",
        "    # Passing constants to the train function\n",
        "    train_fn_with_parameters = tune.with_parameters(\n",
        "        train_mnist_tune,\n",
        "        num_epochs=num_epochs,\n",
        "        num_gpus=gpus_per_trial,\n",
        "        data_dir=data_dir,\n",
        "    )\n",
        "    resources_per_trial = {\"cpu\": 1, \"gpu\": gpus_per_trial}\n",
        "    # Tuning hyperparameters\n",
        "    tuner = tune.Tuner(\n",
        "        tune.with_resources(train_fn_with_parameters, resources=resources_per_trial),\n",
        "        tune_config=tune.TuneConfig(\n",
        "            metric=\"loss\",\n",
        "            mode=\"min\",\n",
        "            scheduler=scheduler,\n",
        "            num_samples=num_samples,\n",
        "        ),\n",
        "        run_config=train.RunConfig(\n",
        "            name=\"tune_mnist_asha\",\n",
        "            progress_reporter=reporter,\n",
        "        ),\n",
        "        param_space=config,\n",
        "    )\n",
        "    results = tuner.fit()\n",
        "\n",
        "    print(\"Best hyperparameters found were: \", results.get_best_result().config)\n"
      ],
      "metadata": {
        "id": "PPj_cauLEwhI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tune_mnist_asha()"
      ],
      "metadata": {
        "id": "oDKqJokCGQHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad6c8c6-d202-4181-dd27-f0967908e6d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-21 14:28:08,901\tINFO worker.py:1770 -- Started a local Ray instance.\n",
            "2024-06-21 14:28:11,562\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-06-21 14:28:12,027\tWARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------+\n",
            "| Configuration for experiment     tune_mnist_asha         |\n",
            "+----------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator   |\n",
            "| Scheduler                        AsyncHyperBandScheduler |\n",
            "| Number of trials                 5                       |\n",
            "+----------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/tune_mnist_asha\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-06-21_14-28-05_437677_314/artifacts/2024-06-21_14-28-11/tune_mnist_asha/driver_artifacts`\n",
            "\n",
            "Trial status: 5 PENDING\n",
            "Current time: 2024-06-21 14:28:12. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                     status       layer_1_size     layer_2_size           lr     batch_size |\n",
            "+-------------------------------------------------------------------------------------------------------+\n",
            "| train_mnist_tune_7ce32_00000   PENDING               128               64   0.0119346              32 |\n",
            "| train_mnist_tune_7ce32_00001   PENDING                64              128   0.0585716              32 |\n",
            "| train_mnist_tune_7ce32_00002   PENDING                64              128   0.00143014             64 |\n",
            "| train_mnist_tune_7ce32_00003   PENDING               128              128   0.00619265             64 |\n",
            "| train_mnist_tune_7ce32_00004   PENDING                64              256   0.00222673            128 |\n",
            "+-------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_mnist_tune_7ce32_00000 started with configuration:\n",
            "+-------------------------------------------------------+\n",
            "| Trial train_mnist_tune_7ce32_00000 config             |\n",
            "+-------------------------------------------------------+\n",
            "| batch_size                                         32 |\n",
            "| layer_1_size                                      128 |\n",
            "| layer_2_size                                       64 |\n",
            "| lr                                            0.01193 |\n",
            "+-------------------------------------------------------+\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m /usr/local/lib/python3.10/dist-packages/ray/tune/integration/pytorch_lightning.py:198: UserWarning: `ray.tune.integration.pytorch_lightning.TuneReportCallback` is deprecated. Use `ray.tune.integration.pytorch_lightning.TuneReportCheckpointCallback` instead.\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m   rank_zero_deprecation(\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m GPU available: True (cuda), used: True\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m IPU available: False, using: 0 IPUs\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Failed to download (trying next):\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m HTTP Error 403: Forbidden\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /root/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \r  0%|          | 0/9912422 [00:00<?, ?it/s]\n",
            "  1%|          | 65536/9912422 [00:00<00:18, 545950.54it/s]\n",
            "  2%|▏         | 163840/9912422 [00:00<00:13, 701520.53it/s]\n",
            "  6%|▋         | 622592/9912422 [00:00<00:04, 2109913.46it/s]\n",
            " 25%|██▍       | 2457600/9912422 [00:00<00:01, 7244788.08it/s]\n",
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16224579.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Extracting /root/data/MNIST/raw/train-images-idx3-ubyte.gz to /root/data/MNIST/raw\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Failed to download (trying next):\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m HTTP Error 403: Forbidden\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /root/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \r  0%|          | 0/28881 [00:00<?, ?it/s]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \r100%|██████████| 28881/28881 [00:00<00:00, 473504.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Extracting /root/data/MNIST/raw/train-labels-idx1-ubyte.gz to /root/data/MNIST/raw\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Failed to download (trying next):\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m HTTP Error 403: Forbidden\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /root/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \r  0%|          | 0/1648877 [00:00<?, ?it/s]\n",
            "  4%|▍         | 65536/1648877 [00:00<00:02, 532550.18it/s]\n",
            " 12%|█▏        | 196608/1648877 [00:00<00:01, 845040.70it/s]\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3812548.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Extracting /root/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/data/MNIST/raw\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Failed to download (trying next):\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m HTTP Error 403: Forbidden\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /root/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \r  0%|          | 0/4542 [00:00<?, ?it/s]\r100%|██████████| 4542/4542 [00:00<00:00, 11088782.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m Extracting /root/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/data/MNIST/raw\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m   | Name    | Type   | Params\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m -----------------------------------\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 0 | layer_1 | Linear | 100 K \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 1 | layer_2 | Linear | 8.3 K \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 2 | layer_3 | Linear | 650   \n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m -----------------------------------\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 109 K     Trainable params\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 0         Non-trainable params\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 109 K     Total params\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 0.438     Total estimated model params size (MB)\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 2024-06-21 14:28:25.246561: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 2024-06-21 14:28:25.246633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 2024-06-21 14:28:25.355722: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 2024-06-21 14:28:25.369265: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m 2024-06-21 14:28:26.719585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity Checking: 0it [00:00, ?it/s]\n",
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  6.34it/s]\n",
            "Epoch 0:   0%|          | 0/1876 [00:00<?, ?it/s] \n",
            "Epoch 0:   1%|          | 20/1876 [00:00<00:48, 38.60it/s, loss=1.66, v_num=.]\n",
            "Epoch 0:   2%|▏         | 40/1876 [00:00<00:35, 51.82it/s, loss=0.695, v_num=.]\n",
            "Epoch 0:   3%|▎         | 60/1876 [00:01<00:30, 58.78it/s, loss=0.591, v_num=.]\n",
            "Epoch 0:   4%|▍         | 80/1876 [00:01<00:28, 62.48it/s, loss=0.505, v_num=.]\n",
            "Epoch 0:   5%|▌         | 100/1876 [00:01<00:27, 64.76it/s, loss=0.529, v_num=.]\n",
            "Epoch 0:   6%|▋         | 120/1876 [00:01<00:26, 67.33it/s, loss=0.52, v_num=.] \n",
            "Epoch 0:   7%|▋         | 140/1876 [00:02<00:25, 69.13it/s, loss=0.416, v_num=.]\n",
            "Epoch 0:   9%|▊         | 160/1876 [00:02<00:24, 70.14it/s, loss=0.517, v_num=.]\n",
            "Epoch 0:  10%|▉         | 180/1876 [00:02<00:23, 71.09it/s, loss=0.431, v_num=.]\n",
            "Epoch 0:  11%|█         | 200/1876 [00:02<00:23, 71.83it/s, loss=0.465, v_num=.]\n",
            "Epoch 0:  12%|█▏        | 220/1876 [00:03<00:22, 72.72it/s, loss=0.352, v_num=.]\n",
            "Epoch 0:  13%|█▎        | 240/1876 [00:03<00:22, 73.11it/s, loss=0.455, v_num=.]\n",
            "Epoch 0:  14%|█▍        | 260/1876 [00:03<00:22, 73.42it/s, loss=0.391, v_num=.]\n",
            "Epoch 0:  15%|█▍        | 280/1876 [00:03<00:21, 73.75it/s, loss=0.441, v_num=.]\n",
            "Epoch 0:  16%|█▌        | 300/1876 [00:04<00:21, 74.06it/s, loss=0.37, v_num=.] \n",
            "Epoch 0:  17%|█▋        | 320/1876 [00:04<00:20, 74.25it/s, loss=0.396, v_num=.]\n",
            "Epoch 0:  18%|█▊        | 340/1876 [00:04<00:20, 74.30it/s, loss=0.303, v_num=.]\n",
            "Epoch 0:  19%|█▉        | 360/1876 [00:04<00:20, 74.54it/s, loss=0.34, v_num=.] \n",
            "Epoch 0:  20%|██        | 380/1876 [00:05<00:20, 74.64it/s, loss=0.417, v_num=.]\n",
            "Epoch 0:  21%|██▏       | 400/1876 [00:05<00:19, 74.72it/s, loss=0.335, v_num=.]\n",
            "Epoch 0:  22%|██▏       | 420/1876 [00:05<00:19, 74.51it/s, loss=0.368, v_num=.]\n",
            "Epoch 0:  23%|██▎       | 440/1876 [00:05<00:19, 74.68it/s, loss=0.331, v_num=.]\n",
            "Epoch 0:  25%|██▍       | 460/1876 [00:06<00:18, 75.01it/s, loss=0.402, v_num=.]\n",
            "Epoch 0:  26%|██▌       | 480/1876 [00:06<00:18, 75.31it/s, loss=0.283, v_num=.]\n",
            "Epoch 0:  27%|██▋       | 500/1876 [00:06<00:18, 75.49it/s, loss=0.318, v_num=.]\n",
            "Epoch 0:  28%|██▊       | 520/1876 [00:06<00:17, 75.68it/s, loss=0.358, v_num=.]\n",
            "Epoch 0:  29%|██▉       | 540/1876 [00:07<00:17, 75.86it/s, loss=0.355, v_num=.]\n",
            "Epoch 0:  30%|██▉       | 560/1876 [00:07<00:17, 75.18it/s, loss=0.243, v_num=.]\n",
            "Epoch 0:  31%|███       | 580/1876 [00:07<00:17, 74.39it/s, loss=0.335, v_num=.]\n",
            "Epoch 0:  32%|███▏      | 600/1876 [00:08<00:17, 73.71it/s, loss=0.403, v_num=.]\n",
            "Epoch 0:  33%|███▎      | 620/1876 [00:08<00:17, 72.98it/s, loss=0.368, v_num=.]\n",
            "Epoch 0:  34%|███▍      | 640/1876 [00:08<00:17, 72.40it/s, loss=0.356, v_num=.]\n",
            "Epoch 0:  35%|███▌      | 660/1876 [00:09<00:16, 71.91it/s, loss=0.269, v_num=.]\n",
            "Epoch 0:  36%|███▌      | 680/1876 [00:09<00:16, 71.62it/s, loss=0.385, v_num=.]\n",
            "Epoch 0:  37%|███▋      | 700/1876 [00:09<00:16, 70.86it/s, loss=0.328, v_num=.]\n",
            "Epoch 0:  38%|███▊      | 720/1876 [00:10<00:16, 70.17it/s, loss=0.421, v_num=.]\n",
            "Epoch 0:  39%|███▉      | 740/1876 [00:10<00:16, 69.63it/s, loss=0.311, v_num=.]\n",
            "Epoch 0:  41%|████      | 760/1876 [00:11<00:16, 68.59it/s, loss=0.292, v_num=.]\n",
            "Epoch 0:  42%|████▏     | 780/1876 [00:11<00:15, 68.94it/s, loss=0.296, v_num=.]\n",
            "Epoch 0:  43%|████▎     | 800/1876 [00:11<00:15, 69.24it/s, loss=0.308, v_num=.]\n",
            "Epoch 0:  44%|████▎     | 820/1876 [00:11<00:15, 69.27it/s, loss=0.309, v_num=.]\n",
            "Epoch 0:  45%|████▍     | 840/1876 [00:12<00:14, 69.19it/s, loss=0.326, v_num=.]\n",
            "Epoch 0:  46%|████▌     | 860/1876 [00:12<00:14, 69.47it/s, loss=0.354, v_num=.]\n",
            "Epoch 0:  47%|████▋     | 880/1876 [00:12<00:14, 69.68it/s, loss=0.239, v_num=.]\n",
            "Epoch 0:  48%|████▊     | 900/1876 [00:12<00:13, 69.84it/s, loss=0.335, v_num=.]\n",
            "Epoch 0:  49%|████▉     | 920/1876 [00:13<00:13, 70.03it/s, loss=0.348, v_num=.]\n",
            "Epoch 0:  50%|█████     | 940/1876 [00:13<00:13, 70.29it/s, loss=0.34, v_num=.] \n",
            "Epoch 0:  51%|█████     | 960/1876 [00:13<00:12, 70.50it/s, loss=0.291, v_num=.]\n",
            "\n",
            "Trial status: 1 RUNNING | 4 PENDING\n",
            "Current time: 2024-06-21 14:28:42. Total running time: 30s\n",
            "Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                     status       layer_1_size     layer_2_size           lr     batch_size |\n",
            "+-------------------------------------------------------------------------------------------------------+\n",
            "| train_mnist_tune_7ce32_00000   RUNNING               128               64   0.0119346              32 |\n",
            "| train_mnist_tune_7ce32_00001   PENDING                64              128   0.0585716              32 |\n",
            "| train_mnist_tune_7ce32_00002   PENDING                64              128   0.00143014             64 |\n",
            "| train_mnist_tune_7ce32_00003   PENDING               128              128   0.00619265             64 |\n",
            "| train_mnist_tune_7ce32_00004   PENDING                64              256   0.00222673            128 |\n",
            "+-------------------------------------------------------------------------------------------------------+\n",
            "Epoch 0:  52%|█████▏    | 980/1876 [00:13<00:12, 70.55it/s, loss=0.406, v_num=.]\n",
            "Epoch 0:  53%|█████▎    | 1000/1876 [00:14<00:12, 70.74it/s, loss=0.254, v_num=.]\n",
            "Epoch 0:  54%|█████▍    | 1020/1876 [00:14<00:12, 70.88it/s, loss=0.33, v_num=.] \n",
            "Epoch 0:  55%|█████▌    | 1040/1876 [00:14<00:11, 71.10it/s, loss=0.292, v_num=.]\n",
            "Epoch 0:  57%|█████▋    | 1060/1876 [00:14<00:11, 71.31it/s, loss=0.279, v_num=.]\n",
            "Epoch 0:  58%|█████▊    | 1080/1876 [00:15<00:11, 71.41it/s, loss=0.315, v_num=.]\n",
            "Epoch 0:  59%|█████▊    | 1100/1876 [00:15<00:10, 71.63it/s, loss=0.224, v_num=.]\n",
            "Epoch 0:  60%|█████▉    | 1120/1876 [00:15<00:10, 71.76it/s, loss=0.286, v_num=.]\n",
            "Epoch 0:  61%|██████    | 1140/1876 [00:15<00:10, 71.87it/s, loss=0.275, v_num=.]\n",
            "Epoch 0:  62%|██████▏   | 1160/1876 [00:16<00:09, 71.98it/s, loss=0.29, v_num=.] \n",
            "Epoch 0:  63%|██████▎   | 1180/1876 [00:16<00:09, 72.12it/s, loss=0.337, v_num=.]\n",
            "Epoch 0:  64%|██████▍   | 1200/1876 [00:16<00:09, 72.22it/s, loss=0.293, v_num=.]\n",
            "Epoch 0:  65%|██████▌   | 1220/1876 [00:16<00:09, 72.29it/s, loss=0.273, v_num=.]\n",
            "Epoch 0:  66%|██████▌   | 1240/1876 [00:17<00:08, 72.34it/s, loss=0.289, v_num=.]\n",
            "Epoch 0:  67%|██████▋   | 1260/1876 [00:17<00:08, 72.49it/s, loss=0.357, v_num=.]\n",
            "Epoch 0:  68%|██████▊   | 1280/1876 [00:17<00:08, 72.61it/s, loss=0.365, v_num=.]\n",
            "Epoch 0:  69%|██████▉   | 1300/1876 [00:17<00:07, 72.74it/s, loss=0.29, v_num=.] \n",
            "Epoch 0:  70%|███████   | 1320/1876 [00:18<00:07, 72.77it/s, loss=0.362, v_num=.]\n",
            "Epoch 0:  71%|███████▏  | 1340/1876 [00:18<00:07, 72.93it/s, loss=0.233, v_num=.]\n",
            "Epoch 0:  72%|███████▏  | 1360/1876 [00:18<00:07, 73.06it/s, loss=0.23, v_num=.] \n",
            "Epoch 0:  74%|███████▎  | 1380/1876 [00:18<00:06, 73.18it/s, loss=0.289, v_num=.]\n",
            "Epoch 0:  75%|███████▍  | 1400/1876 [00:19<00:06, 73.25it/s, loss=0.279, v_num=.]\n",
            "Epoch 0:  76%|███████▌  | 1420/1876 [00:19<00:06, 73.34it/s, loss=0.298, v_num=.]\n",
            "Epoch 0:  77%|███████▋  | 1440/1876 [00:19<00:05, 73.47it/s, loss=0.277, v_num=.]\n",
            "Epoch 0:  78%|███████▊  | 1460/1876 [00:19<00:05, 73.55it/s, loss=0.281, v_num=.]\n",
            "Epoch 0:  79%|███████▉  | 1480/1876 [00:20<00:05, 73.60it/s, loss=0.318, v_num=.]\n",
            "Epoch 0:  80%|███████▉  | 1500/1876 [00:20<00:05, 73.73it/s, loss=0.317, v_num=.]\n",
            "Epoch 0:  81%|████████  | 1520/1876 [00:20<00:04, 73.81it/s, loss=0.34, v_num=.] \n",
            "Epoch 0:  82%|████████▏ | 1540/1876 [00:20<00:04, 73.93it/s, loss=0.243, v_num=.]\n",
            "Epoch 0:  83%|████████▎ | 1560/1876 [00:21<00:04, 73.93it/s, loss=0.298, v_num=.]\n",
            "Epoch 0:  84%|████████▍ | 1580/1876 [00:21<00:04, 73.59it/s, loss=0.334, v_num=.]\n",
            "Epoch 0:  85%|████████▌ | 1600/1876 [00:21<00:03, 73.38it/s, loss=0.23, v_num=.] \n",
            "Epoch 0:  86%|████████▋ | 1620/1876 [00:22<00:03, 73.19it/s, loss=0.281, v_num=.]\n",
            "Epoch 0:  87%|████████▋ | 1640/1876 [00:22<00:03, 72.81it/s, loss=0.259, v_num=.]\n",
            "Epoch 0:  88%|████████▊ | 1660/1876 [00:22<00:02, 72.67it/s, loss=0.305, v_num=.]\n",
            "Epoch 0:  90%|████████▉ | 1680/1876 [00:23<00:02, 72.53it/s, loss=0.33, v_num=.] \n",
            "Epoch 0:  91%|█████████ | 1700/1876 [00:23<00:02, 72.29it/s, loss=0.25, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  92%|█████████▏| 1720/1876 [00:23<00:02, 72.01it/s, loss=0.25, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 0:  93%|█████████▎| 1740/1876 [00:24<00:01, 72.00it/s, loss=0.25, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 0:  94%|█████████▍| 1760/1876 [00:24<00:01, 71.88it/s, loss=0.25, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 0:  95%|█████████▍| 1780/1876 [00:24<00:01, 71.81it/s, loss=0.25, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 0:  96%|█████████▌| 1800/1876 [00:25<00:01, 71.86it/s, loss=0.25, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 0:  97%|█████████▋| 1820/1876 [00:25<00:00, 72.14it/s, loss=0.25, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 0:  98%|█████████▊| 1840/1876 [00:25<00:00, 72.37it/s, loss=0.25, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 0:  99%|█████████▉| 1860/1876 [00:25<00:00, 72.66it/s, loss=0.25, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 0: 100%|██████████| 1876/1876 [00:25<00:00, 72.84it/s, loss=0.26, v_num=.]\n",
            "Epoch 1:   0%|          | 0/1876 [00:00<?, ?it/s, loss=0.26, v_num=.]\n",
            "Epoch 1:   1%|          | 20/1876 [00:00<00:22, 80.76it/s, loss=0.328, v_num=.]\n",
            "Epoch 1:   2%|▏         | 40/1876 [00:00<00:22, 81.78it/s, loss=0.301, v_num=.]\n",
            "Epoch 1:   3%|▎         | 60/1876 [00:00<00:22, 79.71it/s, loss=0.232, v_num=.]\n",
            "Epoch 1:   4%|▍         | 80/1876 [00:00<00:22, 80.99it/s, loss=0.322, v_num=.]\n",
            "Epoch 1:   5%|▌         | 100/1876 [00:01<00:21, 80.74it/s, loss=0.322, v_num=.]\n",
            "Epoch 1:   6%|▋         | 120/1876 [00:01<00:21, 80.83it/s, loss=0.384, v_num=.]\n",
            "Epoch 1:   7%|▋         | 140/1876 [00:01<00:21, 80.80it/s, loss=0.291, v_num=.]\n",
            "Epoch 1:   9%|▊         | 160/1876 [00:01<00:21, 81.10it/s, loss=0.375, v_num=.]\n",
            "Epoch 1:  10%|▉         | 180/1876 [00:02<00:20, 81.17it/s, loss=0.235, v_num=.]\n",
            "Epoch 1:  11%|█         | 200/1876 [00:02<00:20, 81.25it/s, loss=0.298, v_num=.]\n",
            "Epoch 1:  12%|█▏        | 220/1876 [00:02<00:20, 81.05it/s, loss=0.274, v_num=.]\n",
            "Epoch 1:  13%|█▎        | 240/1876 [00:02<00:20, 81.25it/s, loss=0.312, v_num=.]\n",
            "Epoch 1:  14%|█▍        | 260/1876 [00:03<00:19, 81.25it/s, loss=0.271, v_num=.]\n",
            "Epoch 1:  15%|█▍        | 280/1876 [00:03<00:19, 81.06it/s, loss=0.242, v_num=.]\n",
            "Epoch 1:  16%|█▌        | 300/1876 [00:03<00:19, 81.21it/s, loss=0.273, v_num=.]\n",
            "Epoch 1:  17%|█▋        | 320/1876 [00:03<00:19, 80.94it/s, loss=0.31, v_num=.] \n",
            "Epoch 1:  18%|█▊        | 340/1876 [00:04<00:18, 81.02it/s, loss=0.282, v_num=.]\n",
            "Epoch 1:  19%|█▉        | 360/1876 [00:04<00:18, 80.99it/s, loss=0.255, v_num=.]\n",
            "Epoch 1:  20%|██        | 380/1876 [00:04<00:18, 81.02it/s, loss=0.252, v_num=.]\n",
            "Epoch 1:  21%|██▏       | 400/1876 [00:04<00:18, 81.11it/s, loss=0.273, v_num=.]\n",
            "Epoch 1:  22%|██▏       | 420/1876 [00:05<00:17, 81.22it/s, loss=0.288, v_num=.]\n",
            "Epoch 1:  23%|██▎       | 440/1876 [00:05<00:17, 81.14it/s, loss=0.359, v_num=.]\n",
            "Epoch 1:  25%|██▍       | 460/1876 [00:05<00:17, 81.28it/s, loss=0.3, v_num=.]  \n",
            "Epoch 1:  26%|██▌       | 480/1876 [00:05<00:17, 81.15it/s, loss=0.191, v_num=.]\n",
            "Epoch 1:  27%|██▋       | 500/1876 [00:06<00:16, 81.30it/s, loss=0.188, v_num=.]\n",
            "Epoch 1:  28%|██▊       | 520/1876 [00:06<00:16, 81.28it/s, loss=0.327, v_num=.]\n",
            "Epoch 1:  29%|██▉       | 540/1876 [00:06<00:16, 81.38it/s, loss=0.345, v_num=.]\n",
            "Epoch 1:  30%|██▉       | 560/1876 [00:06<00:16, 81.30it/s, loss=0.229, v_num=.]\n",
            "Epoch 1:  31%|███       | 580/1876 [00:07<00:15, 81.27it/s, loss=0.273, v_num=.]\n",
            "Epoch 1:  32%|███▏      | 600/1876 [00:07<00:15, 81.22it/s, loss=0.47, v_num=.] \n",
            "Epoch 1:  33%|███▎      | 620/1876 [00:07<00:15, 81.33it/s, loss=0.373, v_num=.]\n",
            "Epoch 1:  34%|███▍      | 640/1876 [00:07<00:15, 81.06it/s, loss=0.31, v_num=.] \n",
            "Epoch 1:  35%|███▌      | 660/1876 [00:08<00:14, 81.12it/s, loss=0.244, v_num=.]\n",
            "Epoch 1:  36%|███▌      | 680/1876 [00:08<00:14, 81.13it/s, loss=0.295, v_num=.]\n",
            "Epoch 1:  37%|███▋      | 700/1876 [00:08<00:14, 81.20it/s, loss=0.257, v_num=.]\n",
            "Epoch 1:  38%|███▊      | 720/1876 [00:08<00:14, 81.29it/s, loss=0.332, v_num=.]\n",
            "Epoch 1:  39%|███▉      | 740/1876 [00:09<00:13, 81.24it/s, loss=0.202, v_num=.]\n",
            "Epoch 1:  41%|████      | 760/1876 [00:09<00:13, 80.54it/s, loss=0.254, v_num=.]\n",
            "Epoch 1:  42%|████▏     | 780/1876 [00:09<00:13, 79.76it/s, loss=0.28, v_num=.] \n",
            "Epoch 1:  43%|████▎     | 800/1876 [00:10<00:13, 78.75it/s, loss=0.262, v_num=.]\n",
            "Epoch 1:  44%|████▎     | 820/1876 [00:10<00:13, 77.80it/s, loss=0.259, v_num=.]\n",
            "Epoch 1:  45%|████▍     | 840/1876 [00:10<00:13, 77.22it/s, loss=0.295, v_num=.]\n",
            "Epoch 1:  46%|████▌     | 860/1876 [00:11<00:13, 76.45it/s, loss=0.272, v_num=.]\n",
            "Epoch 1:  47%|████▋     | 880/1876 [00:11<00:13, 75.86it/s, loss=0.268, v_num=.]\n",
            "Epoch 1:  48%|████▊     | 900/1876 [00:11<00:12, 75.31it/s, loss=0.29, v_num=.] \n",
            "Epoch 1:  49%|████▉     | 920/1876 [00:12<00:12, 74.58it/s, loss=0.31, v_num=.]\n",
            "Epoch 1:  50%|█████     | 940/1876 [00:12<00:12, 73.89it/s, loss=0.319, v_num=.]\n",
            "Epoch 1:  51%|█████     | 960/1876 [00:13<00:12, 73.29it/s, loss=0.198, v_num=.]\n",
            "Epoch 1:  52%|█████▏    | 980/1876 [00:13<00:12, 73.40it/s, loss=0.33, v_num=.] \n",
            "Epoch 1:  53%|█████▎    | 1000/1876 [00:13<00:11, 73.52it/s, loss=0.263, v_num=.]\n",
            "Epoch 1:  54%|█████▍    | 1020/1876 [00:13<00:11, 73.67it/s, loss=0.323, v_num=.]\n",
            "Epoch 1:  55%|█████▌    | 1040/1876 [00:14<00:11, 73.87it/s, loss=0.286, v_num=.]\n",
            "Epoch 1:  57%|█████▋    | 1060/1876 [00:14<00:11, 73.89it/s, loss=0.21, v_num=.] \n",
            "Epoch 1:  58%|█████▊    | 1080/1876 [00:14<00:10, 74.06it/s, loss=0.331, v_num=.]\n",
            "Epoch 1:  59%|█████▊    | 1100/1876 [00:14<00:10, 74.20it/s, loss=0.2, v_num=.]  \n",
            "Epoch 1:  60%|█████▉    | 1120/1876 [00:15<00:10, 74.36it/s, loss=0.284, v_num=.]\n",
            "Epoch 1:  61%|██████    | 1140/1876 [00:15<00:09, 74.41it/s, loss=0.214, v_num=.]\n",
            "Epoch 1:  62%|██████▏   | 1160/1876 [00:15<00:09, 74.55it/s, loss=0.254, v_num=.]\n",
            "Epoch 1:  63%|██████▎   | 1180/1876 [00:15<00:09, 74.67it/s, loss=0.363, v_num=.]\n",
            "Epoch 1:  64%|██████▍   | 1200/1876 [00:16<00:09, 74.81it/s, loss=0.257, v_num=.]\n",
            "Epoch 1:  65%|██████▌   | 1220/1876 [00:16<00:08, 74.78it/s, loss=0.248, v_num=.]\n",
            "Epoch 1:  66%|██████▌   | 1240/1876 [00:16<00:08, 74.77it/s, loss=0.156, v_num=.]\n",
            "Epoch 1:  67%|██████▋   | 1260/1876 [00:16<00:08, 74.83it/s, loss=0.32, v_num=.] \n",
            "Epoch 1:  68%|██████▊   | 1280/1876 [00:17<00:07, 74.97it/s, loss=0.306, v_num=.]\n",
            "Epoch 1:  69%|██████▉   | 1300/1876 [00:17<00:07, 74.92it/s, loss=0.207, v_num=.]\n",
            "Epoch 1:  70%|███████   | 1320/1876 [00:17<00:07, 75.07it/s, loss=0.272, v_num=.]\n",
            "Epoch 1:  71%|███████▏  | 1340/1876 [00:17<00:07, 75.17it/s, loss=0.271, v_num=.]\n",
            "Trial status: 1 RUNNING | 4 PENDING\n",
            "Current time: 2024-06-21 14:29:12. Total running time: 1min 0s\n",
            "Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 7ce32_00000 with loss=0.3275297284126282 and params={'layer_1_size': 128, 'layer_2_size': 64, 'lr': 0.011934566962302634, 'batch_size': 32}\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                     status       layer_1_size     layer_2_size           lr     batch_size        acc     iter     total time (s)      loss |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_mnist_tune_7ce32_00000   RUNNING               128               64   0.0119346              32   0.917396        1            35.4389   0.32753 |\n",
            "| train_mnist_tune_7ce32_00001   PENDING                64              128   0.0585716              32                                                  |\n",
            "| train_mnist_tune_7ce32_00002   PENDING                64              128   0.00143014             64                                                  |\n",
            "| train_mnist_tune_7ce32_00003   PENDING               128              128   0.00619265             64                                                  |\n",
            "| train_mnist_tune_7ce32_00004   PENDING                64              256   0.00222673            128                                                  |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "Epoch 1:  72%|███████▏  | 1360/1876 [00:18<00:06, 75.27it/s, loss=0.211, v_num=.]\n",
            "Epoch 1:  74%|███████▎  | 1380/1876 [00:18<00:06, 75.26it/s, loss=0.231, v_num=.]\n",
            "Epoch 1:  75%|███████▍  | 1400/1876 [00:18<00:06, 75.37it/s, loss=0.209, v_num=.]\n",
            "Epoch 1:  76%|███████▌  | 1420/1876 [00:18<00:06, 75.51it/s, loss=0.32, v_num=.] \n",
            "Epoch 1:  77%|███████▋  | 1440/1876 [00:19<00:05, 75.59it/s, loss=0.239, v_num=.]\n",
            "Epoch 1:  78%|███████▊  | 1460/1876 [00:19<00:05, 75.63it/s, loss=0.225, v_num=.]\n",
            "Epoch 1:  79%|███████▉  | 1480/1876 [00:19<00:05, 75.71it/s, loss=0.237, v_num=.]\n",
            "Epoch 1:  80%|███████▉  | 1500/1876 [00:19<00:04, 75.79it/s, loss=0.225, v_num=.]\n",
            "Epoch 1:  81%|████████  | 1520/1876 [00:20<00:04, 75.87it/s, loss=0.259, v_num=.]\n",
            "Epoch 1:  82%|████████▏ | 1540/1876 [00:20<00:04, 75.95it/s, loss=0.256, v_num=.]\n",
            "Epoch 1:  83%|████████▎ | 1560/1876 [00:20<00:04, 75.98it/s, loss=0.269, v_num=.]\n",
            "Epoch 1:  84%|████████▍ | 1580/1876 [00:20<00:03, 76.01it/s, loss=0.278, v_num=.]\n",
            "Epoch 1:  85%|████████▌ | 1600/1876 [00:21<00:03, 76.02it/s, loss=0.221, v_num=.]\n",
            "Epoch 1:  86%|████████▋ | 1620/1876 [00:21<00:03, 76.07it/s, loss=0.24, v_num=.] \n",
            "Epoch 1:  87%|████████▋ | 1640/1876 [00:21<00:03, 76.08it/s, loss=0.252, v_num=.]\n",
            "Epoch 1:  88%|████████▊ | 1660/1876 [00:21<00:02, 76.13it/s, loss=0.222, v_num=.]\n",
            "Epoch 1:  90%|████████▉ | 1680/1876 [00:22<00:02, 76.22it/s, loss=0.237, v_num=.]\n",
            "Epoch 1:  91%|█████████ | 1700/1876 [00:22<00:02, 76.24it/s, loss=0.228, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 1720/1876 [00:22<00:02, 76.27it/s, loss=0.228, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 1:  93%|█████████▎| 1740/1876 [00:22<00:01, 76.53it/s, loss=0.228, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 1:  94%|█████████▍| 1760/1876 [00:22<00:01, 76.80it/s, loss=0.228, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 1:  95%|█████████▍| 1780/1876 [00:23<00:01, 76.87it/s, loss=0.228, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 1:  96%|█████████▌| 1800/1876 [00:23<00:00, 76.75it/s, loss=0.228, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Validation DataLoader 0:  64%|██████▎   | 100/157 [00:01<00:00, 87.38it/s]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 1820/1876 [00:23<00:00, 76.75it/s, loss=0.228, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 1:  98%|█████████▊| 1840/1876 [00:23<00:00, 76.76it/s, loss=0.228, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 1:  99%|█████████▉| 1860/1876 [00:24<00:00, 76.81it/s, loss=0.228, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 1: 100%|██████████| 1876/1876 [00:24<00:00, 76.69it/s, loss=0.178, v_num=.]\n",
            "Epoch 2:   0%|          | 0/1876 [00:00<?, ?it/s, loss=0.178, v_num=.]\n",
            "Epoch 2:   1%|          | 20/1876 [00:00<00:34, 54.40it/s, loss=0.24, v_num=.] \n",
            "Epoch 2:   2%|▏         | 40/1876 [00:00<00:30, 59.25it/s, loss=0.239, v_num=.]\n",
            "Epoch 2:   3%|▎         | 60/1876 [00:01<00:30, 59.01it/s, loss=0.263, v_num=.]\n",
            "Epoch 2:   4%|▍         | 80/1876 [00:01<00:31, 56.34it/s, loss=0.312, v_num=.]\n",
            "Epoch 2:   5%|▌         | 100/1876 [00:01<00:31, 55.88it/s, loss=0.317, v_num=.]\n",
            "Epoch 2:   6%|▋         | 120/1876 [00:02<00:32, 54.33it/s, loss=0.242, v_num=.]\n",
            "Epoch 2:   7%|▋         | 140/1876 [00:02<00:31, 54.95it/s, loss=0.257, v_num=.]\n",
            "Epoch 2:   9%|▊         | 160/1876 [00:02<00:30, 57.18it/s, loss=0.362, v_num=.]\n",
            "Epoch 2:  10%|▉         | 180/1876 [00:03<00:28, 58.87it/s, loss=0.234, v_num=.]\n",
            "Epoch 2:  11%|█         | 200/1876 [00:03<00:27, 60.51it/s, loss=0.276, v_num=.]\n",
            "Epoch 2:  12%|█▏        | 220/1876 [00:03<00:26, 61.90it/s, loss=0.242, v_num=.]\n",
            "Epoch 2:  13%|█▎        | 240/1876 [00:03<00:25, 63.16it/s, loss=0.249, v_num=.]\n",
            "Epoch 2:  14%|█▍        | 260/1876 [00:04<00:25, 64.14it/s, loss=0.209, v_num=.]\n",
            "Epoch 2:  15%|█▍        | 280/1876 [00:04<00:24, 64.71it/s, loss=0.295, v_num=.]\n",
            "Epoch 2:  16%|█▌        | 300/1876 [00:04<00:24, 65.30it/s, loss=0.222, v_num=.]\n",
            "Epoch 2:  17%|█▋        | 320/1876 [00:04<00:23, 65.76it/s, loss=0.247, v_num=.]\n",
            "Epoch 2:  18%|█▊        | 340/1876 [00:05<00:23, 66.61it/s, loss=0.267, v_num=.]\n",
            "Epoch 2:  19%|█▉        | 360/1876 [00:05<00:22, 67.17it/s, loss=0.206, v_num=.]\n",
            "Epoch 2:  20%|██        | 380/1876 [00:05<00:22, 67.84it/s, loss=0.235, v_num=.]\n",
            "Epoch 2:  21%|██▏       | 400/1876 [00:05<00:21, 68.39it/s, loss=0.255, v_num=.]\n",
            "Epoch 2:  22%|██▏       | 420/1876 [00:06<00:21, 68.88it/s, loss=0.235, v_num=.]\n",
            "Epoch 2:  23%|██▎       | 440/1876 [00:06<00:20, 69.13it/s, loss=0.265, v_num=.]\n",
            "Epoch 2:  25%|██▍       | 460/1876 [00:06<00:20, 69.49it/s, loss=0.218, v_num=.]\n",
            "Epoch 2:  26%|██▌       | 480/1876 [00:06<00:19, 69.90it/s, loss=0.136, v_num=.]\n",
            "Epoch 2:  27%|██▋       | 500/1876 [00:07<00:19, 70.33it/s, loss=0.15, v_num=.] \n",
            "Epoch 2:  28%|██▊       | 520/1876 [00:07<00:19, 70.70it/s, loss=0.213, v_num=.]\n",
            "Epoch 2:  29%|██▉       | 540/1876 [00:07<00:18, 71.06it/s, loss=0.279, v_num=.]\n",
            "Epoch 2:  30%|██▉       | 560/1876 [00:07<00:18, 71.30it/s, loss=0.19, v_num=.] \n",
            "Epoch 2:  31%|███       | 580/1876 [00:08<00:18, 71.70it/s, loss=0.304, v_num=.]\n",
            "Epoch 2:  32%|███▏      | 600/1876 [00:08<00:17, 72.08it/s, loss=0.387, v_num=.]\n",
            "Epoch 2:  33%|███▎      | 620/1876 [00:08<00:17, 72.26it/s, loss=0.319, v_num=.]\n",
            "Epoch 2:  34%|███▍      | 640/1876 [00:08<00:17, 72.50it/s, loss=0.222, v_num=.]\n",
            "Epoch 2:  35%|███▌      | 660/1876 [00:09<00:16, 72.77it/s, loss=0.278, v_num=.]\n",
            "Epoch 2:  36%|███▌      | 680/1876 [00:09<00:16, 73.02it/s, loss=0.292, v_num=.]\n",
            "Epoch 2:  37%|███▋      | 700/1876 [00:09<00:16, 73.18it/s, loss=0.231, v_num=.]\n",
            "Epoch 2:  38%|███▊      | 720/1876 [00:09<00:15, 73.28it/s, loss=0.299, v_num=.]\n",
            "Epoch 2:  39%|███▉      | 740/1876 [00:10<00:15, 73.51it/s, loss=0.193, v_num=.]\n",
            "Epoch 2:  41%|████      | 760/1876 [00:10<00:15, 73.62it/s, loss=0.289, v_num=.]\n",
            "Epoch 2:  42%|████▏     | 780/1876 [00:10<00:14, 73.77it/s, loss=0.219, v_num=.]\n",
            "Epoch 2:  43%|████▎     | 800/1876 [00:10<00:14, 73.94it/s, loss=0.237, v_num=.]\n",
            "Epoch 2:  44%|████▎     | 820/1876 [00:11<00:14, 74.08it/s, loss=0.321, v_num=.]\n",
            "Epoch 2:  45%|████▍     | 840/1876 [00:11<00:13, 74.23it/s, loss=0.251, v_num=.]\n",
            "Epoch 2:  46%|████▌     | 860/1876 [00:11<00:13, 74.28it/s, loss=0.251, v_num=.]\n",
            "Epoch 2:  47%|████▋     | 880/1876 [00:11<00:13, 74.40it/s, loss=0.205, v_num=.]\n",
            "Epoch 2:  48%|████▊     | 900/1876 [00:12<00:13, 74.55it/s, loss=0.242, v_num=.]\n",
            "Epoch 2:  49%|████▉     | 920/1876 [00:12<00:12, 74.76it/s, loss=0.241, v_num=.]\n",
            "Epoch 2:  50%|█████     | 940/1876 [00:12<00:12, 74.39it/s, loss=0.238, v_num=.]\n",
            "Epoch 2:  51%|█████     | 960/1876 [00:12<00:12, 73.95it/s, loss=0.226, v_num=.]\n",
            "Epoch 2:  52%|█████▏    | 980/1876 [00:13<00:12, 73.55it/s, loss=0.32, v_num=.] \n",
            "Epoch 2:  53%|█████▎    | 1000/1876 [00:13<00:12, 72.97it/s, loss=0.193, v_num=.]\n",
            "Epoch 2:  54%|█████▍    | 1020/1876 [00:14<00:11, 72.65it/s, loss=0.287, v_num=.]\n",
            "Epoch 2:  55%|█████▌    | 1040/1876 [00:14<00:11, 72.37it/s, loss=0.236, v_num=.]\n",
            "Epoch 2:  57%|█████▋    | 1060/1876 [00:14<00:11, 72.09it/s, loss=0.189, v_num=.]\n",
            "Epoch 2:  58%|█████▊    | 1080/1876 [00:15<00:11, 71.48it/s, loss=0.248, v_num=.]\n",
            "Epoch 2:  59%|█████▊    | 1100/1876 [00:15<00:10, 70.92it/s, loss=0.195, v_num=.]\n",
            "Epoch 2:  60%|█████▉    | 1120/1876 [00:15<00:10, 70.42it/s, loss=0.186, v_num=.]\n",
            "Epoch 2:  61%|██████    | 1140/1876 [00:16<00:10, 69.88it/s, loss=0.191, v_num=.]\n",
            "Epoch 2:  62%|██████▏   | 1160/1876 [00:16<00:10, 70.04it/s, loss=0.292, v_num=.]\n",
            "Epoch 2:  63%|██████▎   | 1180/1876 [00:16<00:09, 70.18it/s, loss=0.302, v_num=.]\n",
            "Epoch 2:  64%|██████▍   | 1200/1876 [00:17<00:09, 70.30it/s, loss=0.242, v_num=.]\n",
            "Epoch 2:  65%|██████▌   | 1220/1876 [00:17<00:09, 70.49it/s, loss=0.229, v_num=.]\n",
            "Epoch 2:  66%|██████▌   | 1240/1876 [00:17<00:09, 70.64it/s, loss=0.154, v_num=.]\n",
            "Epoch 2:  67%|██████▋   | 1260/1876 [00:17<00:08, 70.68it/s, loss=0.289, v_num=.]\n",
            "Epoch 2:  68%|██████▊   | 1280/1876 [00:18<00:08, 70.84it/s, loss=0.283, v_num=.]\n",
            "Epoch 2:  69%|██████▉   | 1300/1876 [00:18<00:08, 70.98it/s, loss=0.246, v_num=.]\n",
            "Epoch 2:  70%|███████   | 1320/1876 [00:18<00:07, 71.16it/s, loss=0.295, v_num=.]\n",
            "Epoch 2:  71%|███████▏  | 1340/1876 [00:18<00:07, 71.19it/s, loss=0.231, v_num=.]\n",
            "Epoch 2:  72%|███████▏  | 1360/1876 [00:19<00:07, 71.36it/s, loss=0.182, v_num=.]\n",
            "Epoch 2:  74%|███████▎  | 1380/1876 [00:19<00:06, 71.51it/s, loss=0.274, v_num=.]\n",
            "Epoch 2:  75%|███████▍  | 1400/1876 [00:19<00:06, 71.61it/s, loss=0.194, v_num=.]\n",
            "Epoch 2:  76%|███████▌  | 1420/1876 [00:19<00:06, 71.76it/s, loss=0.291, v_num=.]\n",
            "Epoch 2:  77%|███████▋  | 1440/1876 [00:20<00:06, 71.81it/s, loss=0.226, v_num=.]\n",
            "Epoch 2:  78%|███████▊  | 1460/1876 [00:20<00:05, 71.96it/s, loss=0.231, v_num=.]\n",
            "Epoch 2:  79%|███████▉  | 1480/1876 [00:20<00:05, 72.11it/s, loss=0.184, v_num=.]\n",
            "Epoch 2:  80%|███████▉  | 1500/1876 [00:20<00:05, 72.21it/s, loss=0.27, v_num=.] \n",
            "Epoch 2:  81%|████████  | 1520/1876 [00:21<00:04, 72.29it/s, loss=0.28, v_num=.]\n",
            "Epoch 2:  82%|████████▏ | 1540/1876 [00:21<00:04, 72.40it/s, loss=0.269, v_num=.]\n",
            "Epoch 2:  83%|████████▎ | 1560/1876 [00:21<00:04, 72.49it/s, loss=0.234, v_num=.]\n",
            "Epoch 2:  84%|████████▍ | 1580/1876 [00:21<00:04, 72.59it/s, loss=0.229, v_num=.]\n",
            "Epoch 2:  85%|████████▌ | 1600/1876 [00:22<00:03, 72.49it/s, loss=0.145, v_num=.]\n",
            "Epoch 2:  86%|████████▋ | 1620/1876 [00:22<00:03, 72.57it/s, loss=0.188, v_num=.]\n",
            "Epoch 2:  87%|████████▋ | 1640/1876 [00:22<00:03, 72.71it/s, loss=0.171, v_num=.]\n",
            "Epoch 2:  88%|████████▊ | 1660/1876 [00:22<00:02, 72.80it/s, loss=0.267, v_num=.]\n",
            "Epoch 2:  90%|████████▉ | 1680/1876 [00:23<00:02, 72.83it/s, loss=0.286, v_num=.]\n",
            "Epoch 2:  91%|█████████ | 1700/1876 [00:23<00:02, 72.95it/s, loss=0.156, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  92%|█████████▏| 1720/1876 [00:23<00:02, 73.07it/s, loss=0.156, v_num=.]\n",
            "Trial status: 1 RUNNING | 4 PENDING\n",
            "Current time: 2024-06-21 14:29:42. Total running time: 1min 30s\n",
            "Logical resource usage: 1.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 7ce32_00000 with loss=0.25957971811294556 and params={'layer_1_size': 128, 'layer_2_size': 64, 'lr': 0.011934566962302634, 'batch_size': 32}\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                     status       layer_1_size     layer_2_size           lr     batch_size        acc     iter     total time (s)      loss |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_mnist_tune_7ce32_00000   RUNNING               128               64   0.0119346              32   0.940088        2             59.907   0.25958 |\n",
            "| train_mnist_tune_7ce32_00001   PENDING                64              128   0.0585716              32                                                  |\n",
            "| train_mnist_tune_7ce32_00002   PENDING                64              128   0.00143014             64                                                  |\n",
            "| train_mnist_tune_7ce32_00003   PENDING               128              128   0.00619265             64                                                  |\n",
            "| train_mnist_tune_7ce32_00004   PENDING                64              256   0.00222673            128                                                  |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 2:  93%|█████████▎| 1740/1876 [00:23<00:01, 73.34it/s, loss=0.156, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 2:  94%|█████████▍| 1760/1876 [00:23<00:01, 73.62it/s, loss=0.156, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 2:  95%|█████████▍| 1780/1876 [00:24<00:01, 73.87it/s, loss=0.156, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 2:  96%|█████████▌| 1800/1876 [00:24<00:01, 74.17it/s, loss=0.156, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 2:  97%|█████████▋| 1820/1876 [00:24<00:00, 74.47it/s, loss=0.156, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 2:  98%|█████████▊| 1840/1876 [00:24<00:00, 74.77it/s, loss=0.156, v_num=.]\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 2:  99%|█████████▉| 1860/1876 [00:24<00:00, 75.04it/s, loss=0.156, v_num=.]\n",
            "\n",
            "Trial train_mnist_tune_7ce32_00000 completed after 3 iterations at 2024-06-21 14:29:43. Total running time: 1min 31s\n",
            "+-------------------------------------------------------+\n",
            "| Trial train_mnist_tune_7ce32_00000 result             |\n",
            "+-------------------------------------------------------+\n",
            "| checkpoint_dir_name                                   |\n",
            "| time_this_iter_s                               24.942 |\n",
            "| time_total_s                                   84.849 |\n",
            "| training_iteration                                  3 |\n",
            "| loss                                          0.32802 |\n",
            "| mean_accuracy                                 0.92834 |\n",
            "+-------------------------------------------------------+\n",
            "\u001b[36m(train_mnist_tune pid=1228)\u001b[0m \n",
            "Epoch 2: 100%|██████████| 1876/1876 [00:24<00:00, 75.27it/s, loss=0.156, v_num=.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-21 14:29:43,952\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "2024-06-21 14:29:43,960\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/tune_mnist_asha' in 0.0062s.\n",
            "2024-06-21 14:29:43,971\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
            "Resume experiment with: Tuner.restore(path=\"/root/ray_results/tune_mnist_asha\", trainable=...)\n",
            "2024-06-21 14:29:43,988\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 4 trial(s):\n",
            "- train_mnist_tune_7ce32_00001: FileNotFoundError('Could not fetch metrics for train_mnist_tune_7ce32_00001: both result.json and progress.csv were not found at /root/ray_results/tune_mnist_asha/train_mnist_tune_7ce32_00001_1_batch_size=32,layer_1_size=64,layer_2_size=128,lr=0.0586_2024-06-21_14-28-12')\n",
            "- train_mnist_tune_7ce32_00002: FileNotFoundError('Could not fetch metrics for train_mnist_tune_7ce32_00002: both result.json and progress.csv were not found at /root/ray_results/tune_mnist_asha/train_mnist_tune_7ce32_00002_2_batch_size=64,layer_1_size=64,layer_2_size=128,lr=0.0014_2024-06-21_14-28-12')\n",
            "- train_mnist_tune_7ce32_00003: FileNotFoundError('Could not fetch metrics for train_mnist_tune_7ce32_00003: both result.json and progress.csv were not found at /root/ray_results/tune_mnist_asha/train_mnist_tune_7ce32_00003_3_batch_size=64,layer_1_size=128,layer_2_size=128,lr=0.0062_2024-06-21_14-28-12')\n",
            "- train_mnist_tune_7ce32_00004: FileNotFoundError('Could not fetch metrics for train_mnist_tune_7ce32_00004: both result.json and progress.csv were not found at /root/ray_results/tune_mnist_asha/train_mnist_tune_7ce32_00004_4_batch_size=128,layer_1_size=64,layer_2_size=256,lr=0.0022_2024-06-21_14-28-12')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 1 TERMINATED | 4 PENDING\n",
            "Current time: 2024-06-21 14:29:43. Total running time: 1min 31s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "Current best trial: 7ce32_00000 with loss=0.32802343368530273 and params={'layer_1_size': 128, 'layer_2_size': 64, 'lr': 0.011934566962302634, 'batch_size': 32}\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                     status         layer_1_size     layer_2_size           lr     batch_size        acc     iter     total time (s)       loss |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_mnist_tune_7ce32_00000   TERMINATED              128               64   0.0119346              32   0.928344        3             84.849   0.328023 |\n",
            "| train_mnist_tune_7ce32_00001   PENDING                  64              128   0.0585716              32                                                   |\n",
            "| train_mnist_tune_7ce32_00002   PENDING                  64              128   0.00143014             64                                                   |\n",
            "| train_mnist_tune_7ce32_00003   PENDING                 128              128   0.00619265             64                                                   |\n",
            "| train_mnist_tune_7ce32_00004   PENDING                  64              256   0.00222673            128                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Best hyperparameters found were:  {'layer_1_size': 128, 'layer_2_size': 64, 'lr': 0.011934566962302634, 'batch_size': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KkigP7iNM3xH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}