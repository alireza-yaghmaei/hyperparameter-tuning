{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using Ray package\n",
        "* Hyperpatameter tuning is done using PopulationBasedTraining Scheduler(PBT)\n",
        "* **Model** : a classification model built with lightning\n",
        "* **Dataset used for training the model** : MNIST\n",
        ""
      ],
      "metadata": {
        "id": "qazEaMzmDHoZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOXUg6ZxCaeQ"
      },
      "outputs": [],
      "source": [
        "!pip install \"ray[tune]\" torch torchvision pytorch-lightning==1.9.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.setrecursionlimit(30000)"
      ],
      "metadata": {
        "id": "MuvEcYiTCpAs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from filelock import FileLock\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import os\n"
      ],
      "metadata": {
        "id": "F0Y3viftCqJo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from ray import train, tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from ray.tune.integration.pytorch_lightning import (\n",
        "    TuneReportCallback,\n",
        "    TuneReportCheckpointCallback,\n",
        ")\n"
      ],
      "metadata": {
        "id": "QfRkH8M1Cqln"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Model With Lightning\n",
        "Built a Classification Model Class from LightningModule"
      ],
      "metadata": {
        "id": "dYuvc1VKCzsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "    def __init__(self, config, data_dir=None):\n",
        "        super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "        self.data_dir = data_dir or os.getcwd()\n",
        "\n",
        "        self.layer_1_size = config[\"layer_1_size\"]\n",
        "        self.layer_2_size = config[\"layer_2_size\"]\n",
        "        self.lr = config[\"lr\"]\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "\n",
        "        # mnist images are (1, 28, 28) (channels, width, height)\n",
        "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
        "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
        "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, width, height = x.size()\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        x = self.layer_1(x)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        x = self.layer_2(x)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        x = self.layer_3(x)\n",
        "        x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def cross_entropy_loss(self, logits, labels):\n",
        "        return F.nll_loss(logits, labels)\n",
        "\n",
        "    def accuracy(self, logits, labels):\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        accuracy = correct / len(labels)\n",
        "        return torch.tensor(accuracy)\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x, y = train_batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "\n",
        "        self.log(\"ptl/train_loss\", loss)\n",
        "        self.log(\"ptl/train_accuracy\", accuracy)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y = val_batch\n",
        "        logits = self.forward(x)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        accuracy = self.accuracy(logits, y)\n",
        "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
        "        self.log(\"ptl/val_loss\", avg_loss)\n",
        "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
        "\n",
        "    # def on_validation_epoch_end(self):\n",
        "    #     avg_loss = torch.stack([x[\"val_loss\"] for x in self.validation_step]).mean()\n",
        "    #     avg_acc = torch.stack([x[\"val_accuracy\"] for x in self.validation_step]).mean()\n",
        "    #     self.log(\"ptl/val_loss\", avg_loss)\n",
        "    #     self.log(\"ptl/val_accuracy\", avg_acc)\n",
        "\n",
        "    @staticmethod\n",
        "    def download_data(data_dir):\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "        )\n",
        "        with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
        "            return MNIST(data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    def prepare_data(self):\n",
        "        mnist_train = self.download_data(self.data_dir)\n",
        "\n",
        "        self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.mnist_train, batch_size=int(self.batch_size))\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.mnist_val, batch_size=int(self.batch_size))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "# def train_mnist(config):\n",
        "#     model = LightningMNISTClassifier(config)\n",
        "#     trainer = pl.Trainer(max_epochs=10, enable_progress_bar=True)\n",
        "\n",
        "#     trainer.fit(model)\n"
      ],
      "metadata": {
        "id": "RhdzvQIRCxWp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning the model parameters"
      ],
      "metadata": {
        "id": "0lrIHRGLF03i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "def train_mnist_tune_checkpoint(config, num_epochs=2, num_gpus=0, data_dir=\"~/data\"):\n",
        "    data_dir = os.path.expanduser(data_dir)\n",
        "    kwargs = {\n",
        "        \"max_epochs\": num_epochs,\n",
        "        # If fractional GPUs passed in, convert to int.\n",
        "        \"gpus\": math.ceil(num_gpus),\n",
        "        \"logger\": TensorBoardLogger(save_dir=os.getcwd(), name=\"\", version=\".\"),\n",
        "        \"enable_progress_bar\": False,\n",
        "        \"callbacks\": [\n",
        "            TuneReportCheckpointCallback(\n",
        "                metrics={\"loss\": \"ptl/val_loss\", \"mean_accuracy\": \"ptl/val_accuracy\"},\n",
        "                filename=\"checkpoint\",\n",
        "                on=\"validation_end\",\n",
        "            )\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    checkpoint = train.get_checkpoint()\n",
        "    if checkpoint:\n",
        "        with checkpoint.as_directory() as checkpoint_dir:\n",
        "            kwargs[\"resume_from_checkpoint\"] = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "\n",
        "    model = LightningMNISTClassifier(config=config, data_dir=data_dir)\n",
        "    trainer = pl.Trainer(**kwargs)\n",
        "\n",
        "    trainer.fit(model)\n"
      ],
      "metadata": {
        "id": "qniDxvlgGsXs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_mnist_pbt(num_samples=10, num_epochs=2, gpus_per_trial=0, data_dir=\"~/data\"):\n",
        "    # Configuring the search space\n",
        "    config = {\n",
        "        \"layer_1_size\": tune.choice([32, 64, 128]),\n",
        "        \"layer_2_size\": tune.choice([64, 128, 256]),\n",
        "        \"lr\": 1e-3,\n",
        "        \"batch_size\": 64,\n",
        "    }\n",
        "    # Selecting a scheduler: PBT\n",
        "    scheduler = PopulationBasedTraining(\n",
        "        perturbation_interval=4,\n",
        "        hyperparam_mutations={\n",
        "            \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "            \"batch_size\": [32, 64, 128],\n",
        "        },\n",
        "    )\n",
        "    # Report\n",
        "    reporter = CLIReporter(\n",
        "        parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"],\n",
        "    )\n",
        "    # Tuning hyperparameters\n",
        "    tuner = tune.Tuner(\n",
        "        tune.with_resources(\n",
        "            # Passing constants to the train function\n",
        "            tune.with_parameters(\n",
        "                train_mnist_tune_checkpoint,\n",
        "                num_epochs=num_epochs,\n",
        "                num_gpus=gpus_per_trial,\n",
        "                data_dir=data_dir,\n",
        "            ),\n",
        "            resources={\"cpu\": 1, \"gpu\": gpus_per_trial},\n",
        "        ),\n",
        "        tune_config=tune.TuneConfig(\n",
        "            metric=\"loss\",\n",
        "            mode=\"min\",\n",
        "            scheduler=scheduler,\n",
        "            num_samples=num_samples,\n",
        "        ),\n",
        "        run_config=train.RunConfig(\n",
        "            name=\"tune_mnist_asha\",\n",
        "            progress_reporter=reporter,\n",
        "        ),\n",
        "        param_space=config,\n",
        "    )\n",
        "    results = tuner.fit()\n",
        "\n",
        "    print(\"Best hyperparameters found were: \", results.get_best_result().config)\n"
      ],
      "metadata": {
        "id": "PPj_cauLEwhI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"~/data/\"\n",
        "\n",
        "tune_mnist_pbt(num_samples=5, num_epochs=3, gpus_per_trial=0, data_dir=data_dir)"
      ],
      "metadata": {
        "id": "oDKqJokCGQHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}